# Folio — Political Intelligence System
# Docker Compose configuration for development and production
#
# Usage:
#   make dev          — build and start all services
#   make down         — stop all services
#   make logs         — tail logs
#
# Production:
#   Copy .env.example to .env, fill in real values, then:
#   docker compose --env-file .env up -d

services:
  # ── PostgreSQL with pgvector ────────────────────────────────
  postgres:
    image: pgvector/pgvector:pg16
    container_name: folio-postgres
    environment:
      POSTGRES_USER: folio
      POSTGRES_PASSWORD: folio_dev
      POSTGRES_DB: folio
    ports:
      - "5432:5432"
    volumes:
      - folio_pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U folio -d folio"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ── Ollama LLM Server ──────────────────────────────────────
  ollama:
    image: ollama/ollama
    container_name: folio-ollama
    ports:
      - "11434:11434"
    volumes:
      - folio_ollama:/root/.ollama
    platform: linux/arm64
    deploy:
      resources:
        limits:
          memory: 16G
    restart: unless-stopped

  # ── Ollama Model Init ──────────────────────────────────────
  ollama-init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_started
    volumes:
      - ./scripts/ollama-init.sh:/ollama-init.sh:ro
    entrypoint: ["sh", "/ollama-init.sh"]
    environment:
      - OLLAMA_HOST=http://ollama:11434
    restart: "no"

  # ── Folio API Server ────────────────────────────────────────
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: folio-api
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_USER: folio
      DB_PASS: folio_dev
      DB_NAME: folio
      DB_SSLMODE: disable
      SERVER_PORT: ":8080"
      SERVER_HOST: ""
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_INSTRUCT_MODEL: llama3
      OLLAMA_EMBED_MODEL: nomic-embed-text
      S3_ENDPOINT: ${S3_ENDPOINT:-}
      S3_BUCKET: ${S3_BUCKET:-folio-evidence}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY:-}
      S3_SECRET_KEY: ${S3_SECRET_KEY:-}
      S3_REGION: ${S3_REGION:-us-ashburn-1}
    restart: unless-stopped

  # ── Folio Background Worker ─────────────────────────────────
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: folio-worker
    entrypoint: ["/app/worker"]
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
      ollama-init:
        condition: service_completed_successfully
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_USER: folio
      DB_PASS: folio_dev
      DB_NAME: folio
      DB_SSLMODE: disable
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_INSTRUCT_MODEL: llama3
      OLLAMA_EMBED_MODEL: nomic-embed-text
      S3_ENDPOINT: ${S3_ENDPOINT:-}
      S3_BUCKET: ${S3_BUCKET:-folio-evidence}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY:-}
      S3_SECRET_KEY: ${S3_SECRET_KEY:-}
      S3_REGION: ${S3_REGION:-us-ashburn-1}
    restart: unless-stopped

  # ── Caddy Reverse Proxy ────────────────────────────────────
  caddy:
    image: caddy:2-alpine
    container_name: folio-caddy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./configs/Caddyfile:/etc/caddy/Caddyfile
      - folio_caddy_data:/data
      - folio_caddy_config:/config
    depends_on:
      - api
    restart: unless-stopped

volumes:
  folio_pgdata:
  folio_ollama:
  folio_caddy_data:
  folio_caddy_config:
